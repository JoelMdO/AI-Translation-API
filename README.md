# Ollama API Wrapper

A FastAPI-based wrapper for the Ollama API that provides enhanced features and easy integration.

## Features

- üöÄ FastAPI-based REST API
- üí¨ Chat and Generate endpoints
- üåä Streaming support for real-time responses
- üìã Model management
- üîç Health checking
- üåê CORS support
- üìö Automatic API documentation

## Prerequisites

1. **Install Ollama**: Download and install from [https://ollama.ai/](https://ollama.ai/)
2. **Start Ollama**: Run `ollama serve` in your terminal
3. **Pull a model**: Run `ollama pull llama3.2` (or your preferred model)

## Quick Start

1. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Configure environment** (optional):
   Edit `.env` file to customize settings:

   ```
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_DEFAULT_MODEL=llama3.2
   API_HOST=0.0.0.0
   API_PORT=8000
   ```

3. **Start the API server**:

   ```bash
   python start.py
   ```

   Or directly:

   ```bash
   python main.py
   ```

4. **Access the API**:
   - API: http://localhost:8000
   - Documentation: http://localhost:8000/docs
   - Health check: http://localhost:8000/health

## API Endpoints

### Core Endpoints

- `GET /` - API information
- `GET /health` - Health check
- `GET /models` - List available models

### Chat Endpoints

- `POST /chat` - Chat with a model
- `POST /chat/stream` - Stream chat responses
- `POST /chat/simple` - Simple chat interface

### Generation Endpoints

- `POST /generate` - Generate text
- `POST /generate/stream` - Stream text generation

## Usage Examples

### Simple Chat

```python
import requests

response = requests.post(
    "http://localhost:8000/chat/simple",
    params={"message": "Hello, how are you?"}
)
print(response.json())
```

### Advanced Chat

```python
import requests

data = {
    "model": "llama3.2",
    "messages": [
        {"role": "user", "content": "Tell me a joke"}
    ],
    "temperature": 0.7
}

response = requests.post("http://localhost:8000/chat", json=data)
print(response.json())
```

### Streaming Chat

```python
import requests

data = {
    "model": "llama3.2",
    "messages": [
        {"role": "user", "content": "Write a short story"}
    ]
}

response = requests.post(
    "http://localhost:8000/chat/stream",
    json=data,
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode())
```

## Testing

Run the test script to verify everything is working:

```bash
python test_api.py
```

## Project Structure

```
OllamaAPI/
‚îú‚îÄ‚îÄ main.py              # FastAPI application
‚îú‚îÄ‚îÄ models.py            # Pydantic models
‚îú‚îÄ‚îÄ ollama_client.py     # Ollama API client
‚îú‚îÄ‚îÄ start.py             # Startup script
‚îú‚îÄ‚îÄ test_api.py          # API testing script
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies
‚îú‚îÄ‚îÄ .env                 # Environment configuration
‚îî‚îÄ‚îÄ README.md           # This file
```

## Configuration

The API can be configured using environment variables or the `.env` file:

| Variable               | Default                  | Description                        |
| ---------------------- | ------------------------ | ---------------------------------- |
| `OLLAMA_BASE_URL`      | `http://localhost:11434` | Ollama server URL                  |
| `OLLAMA_DEFAULT_MODEL` | `llama3.2`               | Default model for simple endpoints |
| `API_HOST`             | `0.0.0.0`                | API server host                    |
| `API_PORT`             | `8000`                   | API server port                    |

## Error Handling

The API includes comprehensive error handling:

- Connection errors to Ollama
- Model not found errors
- Invalid request validation
- Timeout handling

## Development

### Running in Development Mode

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### API Documentation

Visit http://localhost:8000/docs for interactive API documentation generated by FastAPI.

## Troubleshooting

1. **Ollama not accessible**: Make sure Ollama is running (`ollama serve`)
2. **Model not found**: Pull the required model (`ollama pull <model_name>`)
3. **Port conflicts**: Change the port in `.env` file
4. **CORS issues**: Configure CORS settings in `main.py`

## License

This project is open source and available under the MIT License.
